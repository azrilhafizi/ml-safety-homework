## Robustness

### Problem 1
Suppose $x_\text{adv} \in \mathbb{R}^d$ and $(x_\text{adv})\_i = x\_i + \epsilon$. What is $||x - x_\text{adv}||_p$ for $p = 1, 2, \infty$? Is the pertubation norm larger for $p = 1$ or $p = \infty$?

### Problem 1 Solution
For $p = 1$: 
```math
||x - x_\text{adv}||_1 = \sum_{i=1}^{d} |(x_i - (x_i + \epsilon))| = \sum_{i=1}^{d} \epsilon = d \cdot \epsilon
```

For $p = 2$: 
```math
||x - x_\text{adv}||_2 = \sqrt{\sum_{i=1}^{d} (x_i - (x_i + \epsilon))^2} = \sqrt{\sum_{i=1}^{d} \epsilon^2} = \sqrt{d} \cdot \epsilon
```

For $p = \infty$: $$||x - x_\text{adv}||_\infty = \max_i |x_i - (x_i + \epsilon)| = \epsilon$$

The perturbation norm is larger for $p = 1$ as the magnitude scales linearly with the dimensionality $d$.

---

### Problem 2
Mixup, AugMix, and PixMix use Beta or Dirichlet distributions. These distributions require hyperparameters. Given the Beta distributions Beta(1, 1), Beta(5, 5), Beta(0.5, 0.5), which distribution is has a peak at its center, has a peak at its corners, and is a uniform distribution?

### Problem 2 Solution
![](img/beta%20distribution.png)
- Peak at center: Beta(5, 5)
- Peak at corners: Beta(0.5, 0.5)
- Uniform: Beta(1, 1)

---

### Problem 3
Suppose $x$ is our original input, $y$ is its original label, $\theta$ is our model parameters, and $L$ is the cross entropy loss. Let $l(x)_i$ be the logit (pre-softmax input) from our model at dimension $i$, given input $x$.

Consider a “MultiTargeted” attack which targets each wrong class out of K possible classes:
```math
\text{For } t \in \{1, \cdots, y - 1, y + 1, \cdots, K\}
```
```math
L(x, y;\theta) \coloneqq l(x)_y - l(x)_t
```
```math
x_{\text{adv}, t} = x + O(t)
```
$x_\text{adv}$ is set to be whichever $x_{\text{adv}, t}$ incurs the greatest loss.

Mark true for all of the following statements that are correct about this attack, and false otherwise.

### Problem 3 Solution
- [**F**] (a) We should set $O(t) = \arg \min_{\delta:||\delta||_{p \leq \epsilon}} L(x + \delta, y; \theta)$

- [**T**] (b) We should set $O(t) = \arg \max_{\delta:||\delta||_{p \leq \epsilon}} L(x + \delta, y; \theta)$

- [**F**] (c) This attack can be at least $K − 1$ times as expensive as an untargeted attack

- [**F**] (d) The MultiTargeted attack has some of the drawbacks of untargeted attacks, e.g., it can cause models to misclassify examples as similar classes

---

### Problem 4
The minimum $L_2$ norm attack $x_\text{adv}$ for a two class classifier is given by $\min_x ||x - x_0||_2$ subject to $w^T x + w_0 = 0$. Assuming $w \neq 0$, which is true?

### Problem 4 Solution
- [] (a) $x_\text{adv} = 0$

- [] (b) $x_\text{adv} = x_0 - (w^T x_0 + w_0) {w \over ||w||_2}$

- [**T**] (c) $x_\text{adv} = x_0 - ({w^T x_0 + w_0 \over ||w||_2}) {w \over ||w||_2}$

- [] (d) $x_\text{adv} = x_0 - (w^T x_0 + w_0) w $

---

### Problem 5
Suppose we use adversarial training to train our network. The adversarial examples are generated through the fast gradient sign method (FGSM) and are within an $\epsilon$ distance (according to the $L_\infty$ norm) from our original points. Mark true for all of the following statements that are correct about the resulting model, and false otherwise.

### Problem 5 Solution
- [**T**] (a) The network is certified to be robust against all adversarial examples that are less than an $\epsilon$ distance (according to the $L_\infty$ norm) from the original training points

- [**F**] (b) Compared to a classifier trained without adversarial training, our classifier will likely have a higher accuracy on non-adversarial examples

- [**T**] (c) Compared to a classifier trained without adversarial training, our classifier will likely have a higher accuracy on adversarial examples generated via FGSM

- [**T**] (d) The classifier is likely to be more robust to adversarial examples generated from FGSM compared to adversarial examples generated through a different method, e.g. PGD

---

### Problem 6
Mark true for all of the following statements that are correct about adversarial examples and adversarial defense, and false otherwise.

### Problem 6 Solution
- [**F**] (a) Suppose we have set up a normal training process and trained a model. In order to defend against adversarial examples, it suffices to first generate a set of adversarial examples from our already trained model, add them to the dataset we have, and then train a new model from scratch using the same training process we already have

- [**T**] (b) When generating adversarial examples to attack a model, if we cannot have access to that model directly, we can train a similar model with the same dataset and generate adversarial examples on this new model. The adversarial examples can sometimes be effective against the original model

- [**T**] (c) Suppose we want to generate an untargeted adversarial example for a given model by maximizing the loss and we want to constrain the norm of our perturbation to be less than $\epsilon$. If we constrain the perturbation’s $L_\infty$ norm to be less than $\epsilon$, we can typically generate an stronger adversarial example that induces greater loss compared to the adversarial example we can generate if we constraint the perturbation’s $L_2$ norm to be less than $\epsilon$

- [**F**] (d) Suppose we first train our model and then make our model non-differentiable by finely quantizing the activations for each layer without changing the model’s prediction. Now since we cannot take gradients with respect to the model input, our model is guaranteed to be safe against any adversarial examples

---

### Problem 7
Which of the following are true?

### Problem 7 Solution
- [**T**] (a) The only difference between FGSM and PGD is that PGD uses multiple steps

- [**F**] (b) Adversaries trying to upload copyrighted content to YouTube do not always need to constrain their content modifications to a small $l_p$ perturbation

- [**F**] (c) Suppose we are training a logistic regression model with $y \in \{-1, 1\}, \min_w \mathbb{E}_{(x,y) \sim p_\text{data}} L(x, y;w)$, where $L(x, y;w) = \log(1 + e^{-yw^Tx})$. Then sign $(\nabla_x L(x, y;w)) = -y \text{ sign}(w)$

- [**F**] (d) Targeted attacks are usually better able to reduce accuracy than untargeted attacks

- [**T**] (e) White box attacks are usually better able to reduce accuracy than black box
attacks

---

### Problem 8
Mark true for all of the following statements that are correct.

### Problem 8 Solution
- [**T**] (a) ImageNet challenge datasets (e.g., ImageNet-C,R,A) contain image classes that are different from the ones in ImageNet. Thus, they are a good stress test of whether models trained on ImageNet can also generalize to different classes

- [**F**] (b) The ANLI dataset consists of adversarial examples for natural language inference that were generated through a GAN

- [**T**] (c) The ImageNet-C paper shows that models can’t be made more robust to Gaussian noise by training them with training data augmented with Gaussian noise

- [**F**] (d) Mixup is used for augmenting language modeling text datasets

- [**T**] (e) PixMix mixes images (possibly fractals) and augmented images together